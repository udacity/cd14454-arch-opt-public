{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciMed | Notebook 2: Architecture Optimization\n",
    "\n",
    "Welcome back to UdaciMed's optimization pipeline! In this notebook, you will implement hardware-aware architectural modifications to achieve our critical memory reduction goals.\n",
    "\n",
    "### Recap: **Optimization challenge**\n",
    "\n",
    "From [Notebook 1](01_baseline_analysis.ipynb), we identified that our ResNet-18 baseline model faces **memory, computation, speed, and throughput challenges**:\n",
    "\n",
    "- **Target FLOP reduction**: >80%\n",
    "- **Target memory usage**: <100MB \n",
    "- **Target throughput**: >2,000 samples/sec \n",
    "- **Target latency**: <3ms\n",
    "\n",
    "while keeping **model sensitivity** >98%.\n",
    "\n",
    "You have outlined an optimization strategy as part of your baseline model analysis, it is now time to develop the architectural optimizations.\n",
    "\n",
    "### **Architecture optimization strategy**\n",
    "\n",
    "You will implement and evaluate at least **3 architecture optimization techniques** out of the ones previously analyzed, as listed below:\n",
    "\n",
    "1. Interpolation Removal\n",
    "2. Depthwise Separable Convolution\n",
    "3. Grouped Convolutions\n",
    "4. Inverted Residual Blocks\n",
    "5. Low-Rank Factorization\n",
    "6. Channel Optimization\n",
    "7. Parameter Sharing\n",
    "\n",
    "Test all implemented techniques separately, and collect results. Then, decide the final optimization strategy (not all need to be enabled!) for the next phase.\n",
    "\n",
    "---\n",
    "\n",
    "Through this notebook, you will:\n",
    "- **Implement >=3 optimization techniques** with modular toggles\n",
    "- **Compare performance impacts** across memory, latency, FLOPs, and throughput\n",
    "- **Validate clinical safety** by monitoring sensitivity metrics\n",
    "- **Create an optimized model** with the most efficient architectural optimization settings\n",
    "\n",
    "**Let's transform our baseline into a production-ready, memory-efficient diagnostic model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "name = 'parameter-sharing'\n",
    "with open(f\"../results/optimization_results_{name}.pkl\", 'rb') as picklefile:\n",
    "# with open(f\"../results/baseline_results.pkl\", 'rb') as picklefile:\n",
    "    contents = pickle.load(picklefile)\n",
    "\n",
    "print(name.upper())\n",
    "pprint(contents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and load baseline results\n",
    "\n",
    "First, let's set up the environment and load our baseline model with analysis from Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project utilities\n",
    "from utils.data_loader import (\n",
    "    load_pneumoniamnist,\n",
    "    get_sample_batch\n",
    ")\n",
    "from utils.model import (\n",
    "    create_baseline_model,\n",
    "    get_model_info,\n",
    "    train_baseline_model,\n",
    "    plot_training_history\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    evaluate_with_multiple_thresholds\n",
    ")\n",
    "from utils.profiling import (\n",
    "    PerformanceProfiler,\n",
    "    measure_time\n",
    ")\n",
    "from utils.visualization import (\n",
    "    plot_batch_size_comparison,\n",
    "    plot_performance_profile,\n",
    "    plot_operation_breakdown\n",
    ")\n",
    "from utils.architecture_optimization import (\n",
    "    apply_interpolation_removal_optimization,\n",
    "    apply_depthwise_separable_optimization,\n",
    "    apply_grouped_convolution_optimization,\n",
    "    apply_inverted_residual_optimization,\n",
    "    apply_lowrank_factorization,\n",
    "    apply_channel_optimization,\n",
    "    apply_parameter_sharing,\n",
    "    create_optimized_model\n",
    ")\n",
    "\n",
    "# Set the device for pytorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available - optimization profiling will be limited\")\n",
    "\n",
    "print(\"Architecture optimization environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility across optimization experiments\n",
    "def set_deterministic_mode(seed=42):\n",
    "    \"\"\"\n",
    "    Enable deterministic mode for consistent benchmarking.\n",
    "    Critical for fair comparison between different techniques.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable for consistent timing\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_deterministic_mode(42)\n",
    "print(\"Deterministic mode enabled for reproducible benchmarking\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why deterministic mode matters**\n",
    ">\n",
    "> Small variations in random initialization can make it difficult to assess the true impact of optimization techniques. Deterministic mode ensures that performance differences reflect architectural changes, not random variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated targets based on baseline analysis findings\n",
    "OPTIMIZATION_TARGETS = {\n",
    "    'memory_mb': 100,                   # MB \n",
    "    'throughput_samples_sec': 2000,     # samples/sec\n",
    "    'latency_ms': 3,                    # ms \n",
    "    'sensitivity_percent': 98,          # %\n",
    "    'flop_reduction_percent': 80        # % \n",
    "}\n",
    "\n",
    "print(\"Optimization targets for production deployment:\")\n",
    "for metric, target in OPTIMIZATION_TARGETS.items():\n",
    "    print(f\"   {metric.replace('_', ' ').title()}: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results from Notebook 1\n",
    "try:\n",
    "    with open('../results/baseline_results.pkl', 'rb') as f:\n",
    "        baseline_results = pickle.load(f)\n",
    "    \n",
    "    print(\"Loaded baseline results from Notebook 1:\")\n",
    "    print(f\"   Model: {baseline_results['model_name']}\")\n",
    "    print(f\"   Parameters: {baseline_results['total_parameters']:,}\")\n",
    "    print(f\"   Model Size: {baseline_results['model_size_mb']:.1f} MB\")\n",
    "    print(f\"   Memory Usage: {baseline_results['memory']['peak_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Inference Time: {baseline_results['timing']['single_sample_ms']:.2f} ms\")\n",
    "    print(f\"   Throughput: {baseline_results['timing']['batch_throughput_samples_per_sec']:.0f} samples/sec\")\n",
    "    print(f\"   Clinical Sensitivity: {baseline_results['eval_results'][0.7]['recall']:.1%}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Baseline results not found. Please run Notebook 1 first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate baseline model and load the trained weights\n",
    "baseline_config = baseline_results['config']\n",
    "baseline_model = create_baseline_model(\n",
    "    num_classes=baseline_config['num_classes'], \n",
    "    input_size=baseline_config['image_size'], \n",
    "    pretrained=False\n",
    ")\n",
    "\n",
    "try:\n",
    "    baseline_model.load_state_dict(torch.load('../results/best_baseline_model.pth', map_location=device))\n",
    "    baseline_model = baseline_model.to(device)\n",
    "    baseline_model.eval()\n",
    "    print(\"Loaded trained baseline model successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"WARNING: Trained model weights not found. Using randomly initialized model.\")\n",
    "    baseline_model = baseline_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset for evaluation\n",
    "test_loader = load_pneumoniamnist(\n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    size=baseline_config['image_size'], \n",
    "    batch_size=baseline_config['batch_size'],\n",
    "    subset_size=baseline_config['subset_size'] * 0.15 if baseline_config['subset_size'] is not None else None\n",
    ")\n",
    "\n",
    "# Get sample batch for profiling\n",
    "sample_images, sample_labels = get_sample_batch(test_loader)\n",
    "sample_images = sample_images.to(device)\n",
    "sample_labels = sample_labels.to(device)\n",
    "\n",
    "print(f\"Test data loaded: {sample_images.shape} batch for profiling\")\n",
    "print(f\"   Batch memory footprint: {sample_images.numel() * sample_images.element_size() / 1024**2:.1f} MB\")\n",
    "print(f\"   Class distribution: {sample_labels.sum().item()}/{len(sample_labels)} positive cases\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure optimization strategy\n",
    "\n",
    "Define which optimizations to apply. \n",
    "\n",
    "Each technique can be independently enabled/disabled, allowing you to understand individual contributions and find the optimal combination for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure your optimization strategy by setting each technique to True/False\n",
    "# Experiment with different combinations to understand their individual and combined impacts\n",
    "\n",
    "OPTIMIZATION_CONFIG = {\n",
    "    # Enable/disable optimizations - start with one technique at a time for analysis\n",
    "    'interpolation_removal': True,          # True/False - Add your value here\n",
    "    'grouped_conv': False,                  # True/False - Add your value here\n",
    "    'depthwise_separable': True,            # True/False - Add your value here\n",
    "    'channel_optimization': False,          # True/False - Add your value here\n",
    "    'inverted_residuals': False,            # True/False - Add your value here\n",
    "    'lowrank_factorization': False,         # True/False - Add your value here\n",
    "    'parameter_sharing': False,             # True/False - Add your value here\n",
    "    \n",
    "    # Execution parameters for training and inference\n",
    "    'memory_format': torch.preserve_format, # [torch.preserve_format, torch.channels_last] - Add your value here\n",
    "    'use_amp': False                        # True/False; Whether to use torch.cuda.amp - Add your value here\n",
    "}\n",
    "\n",
    "print(\"Optimization configuration:\")\n",
    "for technique, enabled in OPTIMIZATION_CONFIG.items():\n",
    "    if isinstance(enabled, bool):\n",
    "        status = \"ENABLED\" if enabled else \"DISABLED\"\n",
    "        print(f\"   {technique.replace('_', ' ').title()}: {status}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tips for optimization strategy**: \n",
    "> \n",
    "> 1. Start with one technique at a time to understand individual contributions. \n",
    "> 2. Experiment with different optimization parameters by updating the parameters set in the optimization function calls in `create_optimized_model()` (less refactoring), or update `create_optimized_model()` to take parameters from _`OPTIMIZATION_CONFIG`_ and set them directly in the config (more seamless experimentation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply architectural optimizations\n",
    "\n",
    "Now let's create the optimized model by applying your selected techniques.\n",
    "\n",
    "**IMPORTANT**: This is where most of your *TODOs* lie, as you need to implement optimization methods and pipeline in `utils/architecture_optimization.ipynb`. You can find hints in the functions' definitions within the utility script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the required optimization methods in `utils/architecture_optimization.ipynb`\n",
    "\n",
    "print(\"Creating optimized model with selected techniques...\")\n",
    "\n",
    "# Apply optimizations using the architecture optimization utility in the correct order (some optimizations build on others)\n",
    "# TODO: Complete the create_optimized_model() function in `utils/architecture_optimization.ipynb`\n",
    "optimized_model = create_optimized_model(baseline_model, OPTIMIZATION_CONFIG)\n",
    "optimized_model = optimized_model.to(device)\n",
    "\n",
    "print(\"\\nOptimized model created successfully!\")\n",
    "print(f\"   Device: {next(optimized_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review new model architecture\n",
    "summary(optimized_model, input_size=(3, baseline_config['image_size'], baseline_config[\"image_size\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **How have the optimizations changed the expected model size in MB?**\n",
    "> \n",
    "> Optimizations can either increase or decrease different memory components for the model. Use the above summary as a sanity check for your optimizations' implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the new memory footprint\n",
    "def count_unique_parameters(model):\n",
    "    \"\"\"Count actual unique parameter tensors (shared params counted once)\"\"\"\n",
    "    unique_params = {}\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_id = id(param)\n",
    "        if param_id not in unique_params:\n",
    "            unique_params[param_id] = {\n",
    "                'tensor': param,\n",
    "                'shape': param.shape,\n",
    "                'elements': param.numel(),\n",
    "                'first_seen': name,\n",
    "                'shared_with': []\n",
    "            }\n",
    "        else:\n",
    "            unique_params[param_id]['shared_with'].append(name)\n",
    "    \n",
    "    total_unique_elements = sum(info['elements'] for info in unique_params.values())\n",
    "    \n",
    "    return total_unique_elements, len(unique_params), unique_params\n",
    "\n",
    "baseline_unique, baseline_objects, _ = count_unique_parameters(baseline_model)\n",
    "optimized_unique, optimized_objects, optimized_params_details = count_unique_parameters(optimized_model)\n",
    "\n",
    "baseline_state_size = sum(p.numel() for p in baseline_model.parameters())\n",
    "optimized_state_size = sum(p.numel() for p in optimized_model.parameters())\n",
    "\n",
    "print(f\"Baseline model:\")\n",
    "print(f\"   State dict size: {baseline_state_size:,} elements\")\n",
    "print(f\"   Unique parameters: {baseline_unique:,} elements\")\n",
    "\n",
    "print(f\"Optimized model:\")\n",
    "print(f\"   State dict size: {optimized_state_size:,} elements\")  # Remove the \"(same as baseline)\" text!\n",
    "print(f\"   Unique parameters: {optimized_unique:,} elements\")\n",
    "\n",
    "reduction = ((baseline_state_size - optimized_state_size) / baseline_state_size * 100)\n",
    "print(f\"Total parameter reduction: {reduction:.1f}%\")\n",
    "\n",
    "n_shared_params = sum([len(param_details['shared_with']) for param_id, param_details in optimized_params_details.items()])\n",
    "print(f\"# Shared parameters: {n_shared_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Do you see the expected parameter reduction?**\n",
    "> \n",
    "> The analysis above showcases both parameter reduction and parameter sharing (if applied). Optimizing model parameters directly contributes to memory savings, but remember: activation memory during inference often dominates, especially with larger input sizes!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize optimized model via transfer learning\n",
    "\n",
    "While we have changed the architecture, we can still transfer knowledge from the trained baseline model to our optimized architecture for any matching layers.\n",
    "\n",
    "Preserving learned features where possible reduces retraining time and maintains clinical performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer compatible weights from baseline to optimized model\n",
    "# This preserves learned features where possible, reducing retraining time\n",
    "\n",
    "def transfer_compatible_weights(source_model, target_model):\n",
    "    \"\"\"\n",
    "    Transfer weights from source model to target model where layer shapes match.\n",
    "    This preserves learned representations in unmodified layers.\n",
    "    \"\"\"\n",
    "    source_dict = dict(source_model.named_parameters())\n",
    "    target_dict = dict(target_model.named_parameters())\n",
    "    \n",
    "    transferred_layers = []\n",
    "    skipped_layers = []\n",
    "\n",
    "    # TODO: Iterate through layers and define if they can be transferred or need skipping\n",
    "    # Hint: After you match layers by name, what parameter attribute can you look at as a good proxy of whether the layer was updated?\n",
    "    # Find a complete list of attributes at https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    return transferred_layers, skipped_layers\n",
    "\n",
    "print(\"Transferring compatible weights from baseline to optimized model...\")\n",
    "transferred, skipped = transfer_compatible_weights(baseline_model, optimized_model)\n",
    "\n",
    "print(f\"\\nTransferred {len(transferred)} compatible layers\")\n",
    "print(f\"Skipped {len(skipped)} modified/new layers\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train/Fine-tune the optimized model\n",
    "\n",
    "The optimized model needs training to learn the weights of new layers defined by the architectural changes.\n",
    "\n",
    "If the number of new layers is low, keep in mind that lower learning rates can help preserve transferred knowledge while adapting to architectural changes for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data for fine-tuning the optimized model\n",
    "# We need both training and validation data to fine-tune the model\n",
    "\n",
    "train_loader = load_pneumoniamnist(\n",
    "    split=\"train\", \n",
    "    download=True, \n",
    "    size=baseline_config['image_size'], \n",
    "    batch_size=baseline_config['batch_size'],\n",
    "    subset_size=baseline_config['subset_size'] * 0.7 if baseline_config['subset_size'] is not None else None\n",
    ")\n",
    "\n",
    "val_loader = load_pneumoniamnist(\n",
    "    split=\"val\", \n",
    "    download=True, \n",
    "    size=baseline_config['image_size'], \n",
    "    batch_size=baseline_config['batch_size'],\n",
    "    subset_size=baseline_config['subset_size'] * 0.15 if baseline_config['subset_size'] is not None else None\n",
    ")\n",
    "\n",
    "print(f\"Training data loaded:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Image size: {baseline_config['image_size']}x{baseline_config['image_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure training parameters\n",
    "# Feel free to update the training regime (learning rate scheduler, optimizer, regularization, ...) in the train_baseline_model() method of `util/model.py`, if desired\n",
    "\n",
    "FINE_TUNING_CONFIG = {\n",
    "    'num_epochs': None,           # Int - Add your value here\n",
    "    'learning_rate': None,        # Float - Add your value here\n",
    "    'lr_step_size': None,         # Int - Add your value here\n",
    "    'weight_decay': None,         # Float - Add your value here\n",
    "    'patience': None              # Int - Add your value here\n",
    "}\n",
    "\n",
    "print(\"Fine-tuning configuration:\")\n",
    "for key, value in FINE_TUNING_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Fine-tune the optimized model\n",
    "print(\"Fine-tuning optimized model...\")\n",
    "optimized_model, fine_tuning_history = train_baseline_model(\n",
    "    optimized_model, train_loader, val_loader, device, FINE_TUNING_CONFIG, save_path=\"../results/optimized_model.pth\",\n",
    ")\n",
    "\n",
    "# Plot fine-tuning progress\n",
    "plot_training_history(fine_tuning_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Indicators of training success_**:\n",
    "> \n",
    "> - **Validation accuracy** should recover to within 1-2% of baseline (target: >98%)\n",
    "> - **Training should converge quickly** (2-5 epochs), especially with transferred weights\n",
    "> - **Loss curves should be smooth** without significant overfitting\n",
    "> \n",
    "> If training struggles, consider: reducing learning rate, increasing epochs, or simplifying optimizations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate clinical performance\n",
    "\n",
    "Critical step: validate that optimizations maintain clinical safety standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clinical performance of the optimized model\n",
    "print(\"Evaluating clinical performance of optimized model...\")\n",
    "\n",
    "# Evaluate optimized model\n",
    "# TODO: Choose the classification thresholds to test\n",
    "thresholds = []  # Add your values here\n",
    "eval_results = evaluate_with_multiple_thresholds(optimized_model, test_loader, device, thresholds)\n",
    "\n",
    "# TODO: Choose the classification threshold for final optimized metrics\n",
    "# Hint: sensitivity >98% should be a must, but also balance specificity to minimize false positives\n",
    "optimized_classification_threshold =  # Add your value here \n",
    "optimized_metrics = eval_results[optimized_classification_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimized model performance against baseline\n",
    "\n",
    "# TODO: Choose the classification threshold for baseline optimized metrics\n",
    "baseline_classification_thr = # Add your value here\n",
    "baseline_metrics = baseline_results['eval_results'][baseline_classification_thr]\n",
    "\n",
    "print(\"BASELINE vs OPTIMIZED PERFORMANCE COMPARISON:\")\n",
    "print(f\"\\n   Sensitivity (Recall):\")\n",
    "print(f\"     Baseline:  {baseline_metrics['recall']:.1%}\")\n",
    "print(f\"     Optimized: {optimized_metrics['recall']:.1%}\")\n",
    "sensitivity_change = (optimized_metrics['recall'] - baseline_metrics['recall']) * 100\n",
    "print(f\"     Change:    {sensitivity_change:+.1f} percentage points\")\n",
    "\n",
    "print(f\"\\n   Overall Accuracy:\")\n",
    "print(f\"     Baseline:  {baseline_metrics['accuracy']:.1%}\")\n",
    "print(f\"     Optimized: {optimized_metrics['accuracy']:.1%}\")\n",
    "accuracy_change = (optimized_metrics['accuracy'] - baseline_metrics['accuracy']) * 100\n",
    "print(f\"     Change:    {accuracy_change:+.1f} percentage points\")\n",
    "\n",
    "print(f\"\\n   AUC-ROC:\")\n",
    "print(f\"     Baseline:  {baseline_metrics['auc']:.3f}\")\n",
    "print(f\"     Optimized: {optimized_metrics['auc']:.3f}\")\n",
    "auc_change = optimized_metrics['auc'] - baseline_metrics['auc']\n",
    "print(f\"     Change:    {auc_change:+.3f}\")\n",
    "\n",
    "# Clinical safety assessment\n",
    "print(f\"\\nCLINICAL SAFETY ASSESSMENT:\")\n",
    "optimized_sensitivity = optimized_metrics['recall']\n",
    "if optimized_sensitivity >= 0.98:\n",
    "    print(f\"   SAFE: Sensitivity {optimized_sensitivity:.1%} meets clinical requirement (>98%)\")\n",
    "else:\n",
    "    print(f\"   WARNING: Sensitivity {optimized_sensitivity:.1%} below clinical requirement (>98%)\")\n",
    "    print(f\"   RECOMMENDATION: Reduce optimization aggressiveness or retrain with sensitivity focus\")\n",
    "\n",
    "if abs(accuracy_change) <= 2.0:\n",
    "    print(f\"   ACCEPTABLE: Accuracy change {accuracy_change:+.1f}% within tolerance (±2%)\")\n",
    "else:\n",
    "    print(f\"   CAUTION: Accuracy change {accuracy_change:+.1f}% exceeds tolerance (±2%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Profile optimized model performance\n",
    "\n",
    "Now let's measure the performance improvements from our optimizations.\n",
    "\n",
    "Quantifying improvements validates that optimizations achieve their intended benefits and helps prioritize techniques for future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the optimized model's inference performance\n",
    "print(\"Profiling optimized model performance...\")\n",
    "\n",
    "# Initialize profiler\n",
    "profiler = PerformanceProfiler(device=str(device), use_amp=OPTIMIZATION_CONFIG[\"use_amp\"])\n",
    "\n",
    "# Profile timing\n",
    "optimized_timing = profiler.profile_inference_time(\n",
    "    model=optimized_model,\n",
    "    input_tensor=sample_images,\n",
    "    num_runs=100,\n",
    "    warmup_runs=10\n",
    ")\n",
    "\n",
    "print(f\"\\nTIMING RESULTS:\")\n",
    "print(f\"   Single Sample Latency: {optimized_timing['single_sample_ms']:.2f} ms\")\n",
    "print(f\"   Batch Throughput: {optimized_timing['batch_throughput_samples_per_sec']:.0f} samples/sec\")\n",
    "print(f\"   Mean Inference Time: {optimized_timing['mean_ms']:.2f} ms\")\n",
    "print(f\"   95th Percentile: {optimized_timing['p95_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile FLOPs of the optimized model\n",
    "optimized_flops = profiler.profile_flops(\n",
    "    model=optimized_model, \n",
    "    input_tensor=sample_images\n",
    ")\n",
    "\n",
    "if 'error' not in optimized_flops:\n",
    "    print(f\"\\nFLOPs Results:\")\n",
    "    print(f\"   Total: {optimized_flops['total_gflops']:.2f} GFLOPs\")\n",
    "    print(f\"   Per Sample: {optimized_flops['gflops_per_sample']:.2f} GFLOPs\")\n",
    "    if 'module_percentage' in optimized_flops and optimized_flops['module_percentage']:\n",
    "        print(f\"\\n   Top Operations (by FLOPs):\")\n",
    "        for module, percentage in list(optimized_flops['module_percentage'].items())[:5]:\n",
    "            gflops = optimized_flops['module_breakdown_gflops'][module]\n",
    "            print(f\"     {module}: {percentage:.1f}% ({gflops:.2f} GFLOPs)\")\n",
    "else:\n",
    "    print(f\"WARNING: FLOPs calculation failed: {optimized_flops['error']}\")\n",
    "    optimized_flops = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile GPU memory usage of the optimized model\n",
    "optimized_memory = profiler.profile_memory_usage(\n",
    "    model=optimized_model,\n",
    "    input_tensor=sample_images\n",
    ")\n",
    "\n",
    "if 'error' not in optimized_memory:\n",
    "    print(f\"\\nMEMORY RESULTS:\")\n",
    "    print(f\"   Peak GPU Memory: {optimized_memory['peak_memory_mb']:.1f} MB\")\n",
    "    print(f\"   Memory Increase: {optimized_memory['memory_increase_mb']:.1f} MB\")\n",
    "    \n",
    "    if 'component_breakdown' in optimized_memory:\n",
    "        components = optimized_memory['component_breakdown']\n",
    "        print(f\"\\n   Memory Component Breakdown:\")\n",
    "        for component, usage in components.items():\n",
    "            print(f\"     {component.replace('_', ' ').title()}: {usage:.1f} MB\")\n",
    "else:\n",
    "    print(f\"WARNING: Memory profiling error: {optimized_memory['error']}\")\n",
    "    optimized_memory = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance improvements against baseline to validate progress toward optimization targets\n",
    "\n",
    "# Get baseline performance metrics\n",
    "baseline_timing = baseline_results['timing']\n",
    "baseline_memory = baseline_results['memory']\n",
    "baseline_flops = baseline_results['flops']\n",
    "\n",
    "print(\"PERFORMANCE IMPROVEMENT ANALYSIS:\")\n",
    "\n",
    "# Timing improvements\n",
    "latency_improvement = baseline_timing['single_sample_ms'] / optimized_timing['single_sample_ms']\n",
    "throughput_improvement = optimized_timing['batch_throughput_samples_per_sec'] / baseline_timing['batch_throughput_samples_per_sec']\n",
    "\n",
    "print(f\"\\nSpeed Improvements:\")\n",
    "print(f\"   Latency Speedup: {latency_improvement:.2f}x ({baseline_timing['single_sample_ms']:.2f}ms → {optimized_timing['single_sample_ms']:.2f}ms)\")\n",
    "\n",
    "# Memory improvements\n",
    "if 'error' not in optimized_memory and 'error' not in baseline_memory:\n",
    "    memory_reduction = (baseline_memory['peak_memory_mb'] - optimized_memory['peak_memory_mb']) / baseline_memory['peak_memory_mb'] * 100\n",
    "    print(f\"\\n Memory Improvements:\")\n",
    "    print(f\"   Memory Reduction: {memory_reduction:.1f}% ({baseline_memory['peak_memory_mb']:.1f}MB → {optimized_memory['peak_memory_mb']:.1f}MB)\")\n",
    "    \n",
    "    # Check target progress\n",
    "    if optimized_memory['peak_memory_mb'] <= OPTIMIZATION_TARGETS['memory_mb']:\n",
    "        print(f\"   TARGET ACHIEVED: {optimized_memory['peak_memory_mb']:.1f}MB ≤ {OPTIMIZATION_TARGETS['memory_mb']}MB\")\n",
    "    else:\n",
    "        remaining_reduction = optimized_memory['peak_memory_mb'] - OPTIMIZATION_TARGETS['memory_mb']\n",
    "        print(f\"   TARGET PROGRESS: {remaining_reduction:.1f}MB reduction still needed\")\n",
    "\n",
    "# Throughput target check\n",
    "print(f\"\\nThroughput Improvements:\")\n",
    "throughput_target_met = \"TARGET ACHIEVED\" if optimized_timing['batch_throughput_samples_per_sec'] >= OPTIMIZATION_TARGETS['throughput_samples_sec'] else \"WARNING: TARGET IN PROGRESS\"\n",
    "print(f\"   Throughput Gain: {throughput_improvement:.2f}x ({baseline_timing['batch_throughput_samples_per_sec']:.0f} → {optimized_timing['batch_throughput_samples_per_sec']:.0f} samples/sec)\")\n",
    "print(f\"   Throughput Target: {throughput_target_met} ({optimized_timing['batch_throughput_samples_per_sec']:.0f} vs {OPTIMIZATION_TARGETS['throughput_samples_sec']} target)\")\n",
    "\n",
    "# Expected FLOP reduction (based on optimizations applied)\n",
    "print(f\"\\nEstimated FLOP Reduction:\")\n",
    "new_ = optimized_flops['total_gflops']\n",
    "\n",
    "flop_reduction_percent = (1 - (optimized_flops['total_gflops'] / baseline_flops['total_gflops'])) * 100\n",
    "flop_target_met = \"TARGET ACHIEVED\" if flop_reduction_percent >= OPTIMIZATION_TARGETS['flop_reduction_percent'] else \"WARNING: TARGET IN PROGRESS\"\n",
    "print(f\"   FLOP Target: {flop_target_met} ({flop_reduction_percent:.2f} vs {OPTIMIZATION_TARGETS['flop_reduction_percent']})\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What if you are missing some targets, but you are confident that your optimization strategy and implementation are sound?**\n",
    "> \n",
    "> If you almost hit deployment targets + maintain clinical safety, proceed to Notebook 3. Hardware deployment often closes the remaining gaps!\n",
    "> If your clinical safety is slightly below threshold and you have already spent too much time trying to parameter tune, summarize what you'd do to hit this target with more time in the final markdown section here and proceed to Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile performance with different batch sizes\n",
    "# TODO: Choose the batch sizes appropriate for T4 memory constraints\n",
    "batch_sizes = []  # Add your values here\n",
    "\n",
    "print(\"   Profiling multiple batch sizes...\")\n",
    "batch_results = profiler.profile_multiple_batch_sizes(\n",
    "    optimized_model, sample_images.shape, batch_sizes\n",
    ")\n",
    "\n",
    "# Visualize batch size analysis for deployment understanding\n",
    "plot_batch_size_comparison(batch_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize performance improvements\n",
    "\n",
    "Create visual comparisons to understand the optimization impact.\n",
    "\n",
    "Visual analysis helps identify bottlenecks, validate improvements, and communicate results to stakeholders who may not be familiar with raw performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimized model's performance profile, useful to understand where the performance gains come from\n",
    "\n",
    "print(\"Generating performance visualizations...\")\n",
    "plot_performance_profile(optimized_timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed operation breakdown for the optimized model\n",
    "print(\"Analyzing operation distribution changes...\")\n",
    "optimized_detailed = profiler.profile_with_pytorch_profiler(\n",
    "    model=optimized_model,\n",
    "    input_tensor=sample_images,\n",
    "    num_steps=10\n",
    ")\n",
    "\n",
    "if 'error' not in optimized_detailed and 'operation_breakdown' in optimized_detailed:\n",
    "    print(\"\\nOptimized Model Operation Breakdown:\")\n",
    "    op_breakdown = optimized_detailed['operation_breakdown']\n",
    "    \n",
    "    # Show top operations, if they contribute to at least 1% of operations\n",
    "    sorted_ops = sorted(op_breakdown.items(), key=lambda x: x[1], reverse=True)\n",
    "    for op_type, percentage in sorted_ops:\n",
    "        if percentage > 1: \n",
    "            print(f\"   {op_type.replace('_', ' ').title()}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Visualize operation breakdown\n",
    "    plot_operation_breakdown(op_breakdown)\n",
    "else:\n",
    "    print(\"WARNING: Operation breakdown visualization not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison visualization of before/after metrics\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Memory Usage with Target Line\n",
    "models = ['Baseline', 'Optimized']\n",
    "if 'error' not in optimized_memory:\n",
    "    # TODO: Extract baseline and optimized models' memory \n",
    "    # HINT: Which values should you select from baseline_memory and optimized_memory? They should match by name!\n",
    "    memories = []  # Add your values here\n",
    "    colors = ['#e74c3c', '#2ecc71'] \n",
    "    \n",
    "    bars = ax1.bar(models, memories, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "    ax1.set_title('Memory Usage vs Target', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('Memory (MB)', fontsize=12)\n",
    "    ax1.axhline(y=OPTIMIZATION_TARGETS['memory_mb'], color='orange', linestyle='--', linewidth=2, label=f'Target: {OPTIMIZATION_TARGETS[\"memory_mb\"]}MB')\n",
    "    ax1.legend(fontsize=10)\n",
    "    \n",
    "    for bar, memory in zip(bars, memories):\n",
    "        color = 'green' if memory <= OPTIMIZATION_TARGETS['memory_mb'] else 'red'\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                f'{memory:.1f}MB', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "# 2. Throughput with Target Line  \n",
    "# TODO: Extract baseline and optimized models' throughput\n",
    "# HINT: Which values should you select from baseline_timing and optimized_timing? They should match by name!\n",
    "throughputs = []  # Add your values here\n",
    "bars = ax2.bar(models, throughputs, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "ax2.set_title('Throughput vs Target', fontweight='bold', fontsize=14)\n",
    "ax2.set_ylabel('Samples/Sec', fontsize=12)\n",
    "ax2.axhline(y=OPTIMIZATION_TARGETS['throughput_samples_sec'], color='orange', linestyle='--', linewidth=2, label=f'Target: {OPTIMIZATION_TARGETS[\"throughput_samples_sec\"]} samples/sec')\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "for bar, throughput in zip(bars, throughputs):\n",
    "    color = 'green' if throughput >= OPTIMIZATION_TARGETS['throughput_samples_sec'] else 'red'\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "            f'{throughput:.0f}', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "# 3. FLOP Reduction (if available)\n",
    "# TODO: Extract target and optimized model's flops reduction (%)\n",
    "# HINT: Which values should you select from OPTIMIZATION_TARGETS and where did we define flop reduction for the optimized model?\n",
    "flop_data = []   # Add your values here\n",
    "bars = ax3.bar(models, flop_data, color=['#95a5a6', colors[1]], alpha=0.7, edgecolor='black', linewidth=1)\n",
    "ax3.set_title('FLOP Reduction vs Target', fontweight='bold', fontsize=14)\n",
    "ax3.set_ylabel('FLOP Reduction (%)', fontsize=12)\n",
    "ax3.axhline(y=OPTIMIZATION_TARGETS['flop_reduction_percent'], color='orange', linestyle='--', linewidth=2, label=f'Target: {OPTIMIZATION_TARGETS[\"flop_reduction_percent\"]}%')\n",
    "ax3.legend(fontsize=10)\n",
    "\n",
    "for bar, flop_red in zip(bars, flop_data):\n",
    "    if flop_red > 0:\n",
    "        color = 'green' if flop_red >= OPTIMIZATION_TARGETS['flop_reduction_percent'] else 'red'\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{flop_red:.1f}%', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "# 4. Clinical Performance with Target Line\n",
    "# TODO: Extract baseline and optimized models' sensitivities\n",
    "# HINT: Which values should you select from baseline_metrics and optimized_metrics? They should match by name!\n",
    "sensitivities = []  # Add your values here\n",
    "bars = ax4.bar(models, sensitivities, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "ax4.set_title('Clinical Safety vs Target', fontweight='bold', fontsize=14)\n",
    "ax4.set_ylabel('Sensitivity (%)', fontsize=12)\n",
    "ax4.set_ylim(85, 100)\n",
    "ax4.axhline(y=OPTIMIZATION_TARGETS['sensitivity_percent'], color='red', linestyle='--', linewidth=2, label=f'Clinical Requirement: {OPTIMIZATION_TARGETS[\"sensitivity_percent\"]}%')\n",
    "ax4.legend(fontsize=10)\n",
    "\n",
    "for bar, sensitivity in zip(bars, sensitivities):\n",
    "    color = 'green' if sensitivity >= OPTIMIZATION_TARGETS['sensitivity_percent'] else 'red'\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "            f'{sensitivity:.1f}%', ha='center', va='bottom', fontweight='bold', color=color)\n",
    "\n",
    "plt.suptitle('Architecture Optimization Results vs Production Targets', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save optimized model's performance\n",
    "\n",
    "Save the optimized model performance results for future reference, both at deployment planning and for future optimization iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized model weights and optimization results\n",
    "\n",
    "# TODO: Choose a meaningful name for the experiment, which will be used as part of the filename for the results\n",
    "# Hint: Consider having the optimizations applied explicitly mentioned within the experiment name\n",
    "experiment_name =  # Add your code here\n",
    "\n",
    "# Compile optimization results\n",
    "optimization_results = {\n",
    "    'model_name': 'ResNet-18 Optimized',\n",
    "    'optimization_config': OPTIMIZATION_CONFIG,\n",
    "    'clinical_performance': {\n",
    "        'baseline': {\n",
    "            'accuracy': baseline_metrics['accuracy'],\n",
    "            'sensitivity': baseline_metrics['recall'],\n",
    "            'auc': baseline_metrics['auc']\n",
    "        },\n",
    "        'optimized': {\n",
    "            'accuracy': optimized_metrics['accuracy'],\n",
    "            'sensitivity': optimized_metrics['recall'],\n",
    "            'auc': optimized_metrics['auc']\n",
    "        }\n",
    "    },\n",
    "    'performance_improvements': {\n",
    "        'latency_speedup': latency_improvement,\n",
    "        'throughput_improvement': throughput_improvement,\n",
    "        'memory_reduction_percent': memory_reduction if 'error' not in optimized_memory else 0,\n",
    "        'flop_reduction_percent': flop_reduction_percent\n",
    "    },\n",
    "    'timing_results': optimized_timing,\n",
    "    'memory_results': optimized_memory,\n",
    "    'flop_results': optimized_flops,\n",
    "    'operation_breakdown': optimized_detailed.get('operation_breakdown', {}),\n",
    "    'fine_tuning_history': fine_tuning_history\n",
    "}\n",
    "\n",
    "# Save optimization results\n",
    "with open(f'../results/optimization_results_{experiment_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(optimization_results, f)\n",
    "\n",
    "print(f\"Saved optimization results to the '../results/' folder with name optimization_results_{experiment_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!** \n",
    "\n",
    "You have successfully implemented architectural optimizations! Let's recap your progress with architectural optimizations as you move towards the production deployment targets.\n",
    "\n",
    "### Experimental results\n",
    "\n",
    "_<<TODO: Document your systematic experimentation across different technique combinations>>_\n",
    "\n",
    "| Experiment | Techniques Enabled | Parameters | Memory (MB) | Throughput (samples/sec) | Latency (ms) | FLOP Reduction (%) | Sensitivity (%) | Targets Met |\n",
    "|------------|-------------------|------------|-------------|-------------------------|--------------|-------------------|----------------|-------------|\n",
    "| **Baseline** | None | [X] | [X] | [X] | [X] | 0 | [X] | [X]/5 |\n",
    "| **Exp 1** | ... | [X]M | [X] | [X] | [X] | [X] | [X] | [X]/5 |\n",
    "\n",
    "#### Key insights and learnings\n",
    "\n",
    "_<<TODO: Collect relevant insights you have gathered from experimentation including:<br>_\n",
    "_- Most impactful single technique<br>_\n",
    "_- Any unexpected findings<br>_\n",
    "_- Diminishing returns observed<br>_\n",
    "_- Alignment with notebook 1 predictions<br>_\n",
    "_- Clinical safety considerations<br>_\n",
    "_- Additional experimentation given more time>>_\n",
    "\n",
    "### Final optimizations strategy\n",
    "\n",
    "_<<TODO: Document your final optimization choice and reasoning>>_\n",
    "\n",
    "_<<TODO: Summarize your production readiness>>_\n",
    "\n",
    "**Overall Production Readiness**: [X/5] targets achieved → **[Ready for Hardware Acceleration / Needs Additional Work]**\n",
    "\n",
    "_<<TODO: Explain recovery potential with the next hardware acceleration phase>>_\n",
    "\n",
    "### Quiz question: Multi-label classification adaptation\n",
    "\n",
    "**How would your optimization strategy change if the task was multi-label classification from chest X-rays (as in [ChestMNIST](https://github.com/rsm-13/classifying-chestMNIST) which has 14 labels including cardiomegaly, mass, pneumonia, ...) instead of binary classification?**\n",
    "\n",
    "_<<TODO: Briefly analyze the main implications of multi-label vs binary classification for architectural optimization>>_\n",
    "\n",
    "---\n",
    "\n",
    "**You are now ready to move to Notebook 3: Deployment Acceleration!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
