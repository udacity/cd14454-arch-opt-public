{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciMed | Notebook 3: Hardware Acceleration & Production Deployment\n",
    "\n",
    "Welcome to the final phase of UdaciMed's optimization pipeline! In this notebook, you will implement hardware acceleration techniques and deploy your optimized model using production-grade inference infrastructure.\n",
    "\n",
    "## Recap: Optimization Journey\n",
    "\n",
    "In [Notebook 2](02_architecture_optimization.ipynb), you have implemented architectural optimizations that brougth you closer to your optimization targets.\n",
    "\n",
    "Now, it is time to unlock further performance opportunities with hardware acceleration.\n",
    "\n",
    "> **Your mission**: Transform your optimized model into a production-ready deployment that serves multiple healthcare systems simultaneously while maintaining clinical safety standards.\n",
    "\n",
    "### Hardware acceleration\n",
    "\n",
    "You will implement and evaluate **2 core deployment techniques\\***:\n",
    "\n",
    "1. **Mixed Precision (FP16)** - GPU tensor core acceleration\n",
    "2. **Dynamic Batching** - Multi-tenant optimization via NVIDIA Triton\n",
    "\n",
    "with recommended **TensorRT Optimization**.\n",
    "\n",
    "Additionally, you will analyze two other deployment scenarios: CPU (OpenVINO) and Edge deployment considerations.\n",
    "\n",
    "_\\* Note that while you are expected to implement both deployment techniques, you can decide whether to keep either or both in your final deployment strategy to best achieve targets._\n",
    "\n",
    "---\n",
    "\n",
    "Through this notebook, you will:\n",
    "\n",
    "- **Convert PyTorch model to ONNX** for cross-platform deployment\n",
    "- **Deploy via NVIDIA Triton Inference Server** with optional TensorRT optimization\n",
    "- **Benchmark end-to-end performance** using Triton's metrics APIs and Triton Model Analyzer\n",
    "- **Validate clinical safety** across the deployment pipeline\n",
    "- **Analyze alternative deployment strategies** for diverse hardware environments\n",
    "\n",
    "**Let's create a production-ready, hardware-accelerated diagnostic deployment!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup the environment\n",
    "\n",
    "First, let's set up the environment and understand our hardware capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as amp\n",
    "import tensorrt as trt\n",
    "import tritonclient.http as httpclient\n",
    "from prometheus_client.parser import text_string_to_metric_families\n",
    "import numpy as np\n",
    "import onnx\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import json\n",
    "import time\n",
    "import socket\n",
    "import subprocess\n",
    "import requests\n",
    "import docker\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from typing import Dict, List, Optional, Tuple, Any, Literal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project utilities\n",
    "from utils.data_loader import (\n",
    "    load_pneumoniamnist,\n",
    "    get_sample_batch\n",
    ")\n",
    "from utils.model import (\n",
    "    create_baseline_model,\n",
    "    get_model_info\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    evaluate_with_multiple_thresholds\n",
    ")\n",
    "from utils.profiling import (\n",
    "    PerformanceProfiler,\n",
    "    measure_time\n",
    ")\n",
    "from utils.visualization import (\n",
    "    plot_performance_profile,\n",
    "    plot_batch_size_comparison\n",
    ")\n",
    "from utils.architecture_optimization import (\n",
    "    create_optimized_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and analyze hardware capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Check tensor core support for mixed precision - crucial for FP16 acceleration\n",
    "    gpu_compute = torch.cuda.get_device_properties(0).major\n",
    "    tensor_core_support = gpu_compute >= 7  # Volta+ architecture\n",
    "    print(f\"Tensor Core Support: {tensor_core_support}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available - hardware acceleration will be limited\")\n",
    "\n",
    "print(\"Hardware acceleration environment ready!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Hardware check**: Understanding your GPU's tensor core capabilities is crucial for mixed precision decisions. Tensor cores provide significant FP16 acceleration but require Volta+ architecture (compute capability ≥7.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load test data and optimized model with configuration\n",
    "\n",
    "The model is needed for deployment, and the optimization results for comparison.\n",
    "\n",
    "Test data is needed for both conversion and final performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset loading parameters\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Load test dataset for final evaluation\n",
    "test_loader = load_pneumoniamnist(\n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=None\n",
    ")\n",
    "\n",
    "# Get sample batch for profiling\n",
    "sample_images, sample_labels = get_sample_batch(test_loader)\n",
    "sample_images = sample_images.to(device)\n",
    "sample_labels = sample_labels.to(device)\n",
    "\n",
    "print(f\"Test data loaded: {sample_images.shape} batch for hardware acceleration profiling\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch size strategy**: Your batch size choice impacts memory usage, latency, and throughput. \n",
    "> \n",
    "> Consider: What batch size balances efficiency gains with memory constraints for multi-tenant deployment? Don't forget to review the batch analysis plot from Notebook 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized model and results from notebook 2\n",
    "\n",
    "# TODO: Define the experiment name\n",
    "experiment_name = # String - Add your value here\n",
    "\n",
    "with open(f'../results/optimization_results_{experiment_name}.pkl', 'rb') as f:\n",
    "    optimization_results = pickle.load(f)\n",
    "\n",
    "print(\"Loaded optimization results from Notebook 2:\")\n",
    "print(f\"   Model: {optimization_results['model_name']}\")\n",
    "print(f\"   Clinical Performance: {optimization_results['clinical_performance']['optimized']['sensitivity']:.1%} sensitivity\")\n",
    "print(f\"   Architecture Speedup: {optimization_results['performance_improvements']['latency_speedup']:.2f}x\")\n",
    "print(f\"   Memory Reduction: {optimization_results['performance_improvements']['memory_reduction_percent']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HINT: Finding your optimization results**\n",
    "> \n",
    "> Your optimization results from Notebook 2 should be saved as:\n",
    "> - Results file: `../results/optimization_results_{experiment_name}.pkl`\n",
    "> - Model weights: `../results/optimized_model.pth`\n",
    "> \n",
    "> The experiment name typically combines your optimization techniques, like:\n",
    "> - `\"interpolation-removal_depthwise-separable\"`\n",
    "> - `\"channel-reduction_grouped-conv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimization configuration\n",
    "opt_config = optimization_results['optimization_config']\n",
    "optimized_model = None  \n",
    "\n",
    "# TODO: Load the optimized model in the optimized_model variable\n",
    "# HINT: This involves:\n",
    "# > 1. Recreate the baseline model\n",
    "# > 2. Applying the same architectural modifications using the saved optimization configuration\n",
    "# > 3. Loading the trained weights\n",
    "# See https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference for inspiration\n",
    "\n",
    "# Add your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert model for production deployment\n",
    "\n",
    "Convert the optimized model to [ONNX (Open Neural Network Exchange)](https://onnx.ai/). ONNX is the industry standard for model deployment because:\n",
    " - **Cross-platform compatibility**: Works with different inference engines\n",
    " - **Hardware optimization**: Enables automatic optimizations (TensorRT, OpenVINO)\n",
    " - **Production readiness**: Stable format for deployment pipelines\n",
    "\n",
    "Review https://docs.pytorch.org/docs/stable/onnx.html for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your deployment configuration\n",
    "use_fp16 =  # Float; Whether or not to use mixed precision - Add your value here\n",
    "backend =  # String; One of onnxruntime or tensorrt - Add your value here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Mixed precision decision point**: You can implement FP16 mixed precision at multiple stages:\n",
    "> 1. **PyTorch level**: Convert model weights before ONNX export\n",
    "> 2. **ONNX level**: Use FP16 data types in ONNX graph\n",
    "> 3. **TensorRT level**: Apply FP16 during engine optimization\n",
    "> \n",
    "> Each approach has different memory and performance implications. Consider which aligns with your deployment targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch model to ONNX format (for cross-platform deployment)\n",
    "\n",
    "def export_model_to_onnx(model: nn.Module, input_tensor: torch.Tensor, \n",
    "                        export_path: str, model_name: str = \"pneumonia_detection\", fp16_mode: bool = use_fp16) -> str:\n",
    "    \"\"\"\n",
    "    Export PyTorch model to ONNX format for production deployment.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to export\n",
    "        input_tensor: Sample input tensor for shape inference\n",
    "        export_path: Directory to save the ONNX model\n",
    "        model_name: Name for the exported model\n",
    "        fp16_mode: Whether to convert to fp16\n",
    "        \n",
    "    Returns:\n",
    "        Path to exported ONNX model\n",
    "    \"\"\"\n",
    "    # Define output path, and ensure it exists\n",
    "    onnx_path = f\"{export_path}/{model_name}.onnx\"\n",
    "    Path(export_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # TODO: Convert PyTorch model to ONNX format for cross-platform deployment\n",
    "    # HINT: ONNX provides compatibility with TensorRT, OpenVINO, and other inference engines\n",
    "    # Use torch.onnx.export with proper input shapes (how do you enable dynamic batching?) and opset version\n",
    "    # If implementing fp16, think about whether conversion should happen in PyTorch, ONNX, and/or TensorRT\n",
    "\n",
    "    # Add your code here\n",
    "\n",
    "    # Verify ONNX model integrity - sanity check\n",
    "    try:\n",
    "        onnx_model = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        print(\"   ONNX model verification passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"   WARNING: ONNX verification failed: {str(e)}\")\n",
    "\n",
    "    return onnx_path\n",
    "\n",
    "# Export the mixed precision model to ONNX\n",
    "onnx_model_path = export_model_to_onnx(\n",
    "    model=optimized_model,\n",
    "    input_tensor=sample_images,\n",
    "    export_path=\"../results/onnx_models\",\n",
    "    model_name=\"udacimed_pneumonia_optimized\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy with NVIDIA Triton Inference Server\n",
    "\n",
    "Set up production-grade inference server with dynamic batching for multi-tenant scenarios and [TensorRT](https://developer.nvidia.com/tensorrt) acceleration.\n",
    "\n",
    "[NVIDIA Triton Inference Server](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) is the industry standard for AI model deployment because:\n",
    " - **Multi-framework support**: ONNX, TensorRT, PyTorch, TensorFlow\n",
    " - **Automatic optimization**: Built-in TensorRT acceleration for ONNX models\n",
    " - **Production features**: Dynamic batching, model versioning, metrics\n",
    " - **Scalability**: Multi-GPU, multi-model serving\n",
    "\n",
    "Triton provides the infrastructure needed for UdaciMed's multi-tenant requirements, including automatic batching, load balancing, and performance monitoring that individual healthcare systems need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Ensure docker is installed\n",
    "\n",
    "Docker should be installed in your environment already! So, you can just check if the software is installed by calling it.\n",
    "\n",
    "If **docker** is not installed, you can install it following [official instructions](https://docs.docker.com/engine/install/) but only if you have root access to the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test docker is installed\n",
    "! docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Create the Triton model repository structure\n",
    "\n",
    "Triton requires a specific directory structure with model configs. See https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html for details.\n",
    "\n",
    "**Key configuration elements:**\n",
    "- `platform`: Tells Triton which backend to use\n",
    "- `input/output`: Tensor specifications (name, data type, dimensions). _Should these be FP32 or FP16?_\n",
    "- `dynamic_batching`: Enables automatic batching for efficiency\n",
    "- `optimization`: TensorRT acceleration settings.\n",
    "\n",
    "Find additional information specific to the chosen platform in the [`Backends`](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/backend/README.html) documentation section of the Triton Inference server docs.\n",
    "\n",
    "_While it is more common to create the model structure manually, we provide here a guided programming solution._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name in a variable, as it will be required across deployment steps\n",
    "model_name = \"udacimed_pneumonia_production\" \n",
    "\n",
    "# Create the Triton model repository structure programmatically\n",
    "def create_triton_model_repository(\n",
    "    model_name: str = \"udacimed_pneumonia\",\n",
    "    source_model_path: str = None,\n",
    "    backend: str = \"tensorrt\",\n",
    "    fp16_mode: bool = use_fp16,\n",
    "    repository_base: str = \"../deployment/\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create NVIDIA Triton model repository with ONNX or TensorRT support.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name for the model in Triton\n",
    "        source_model_path: Path to ONNX model or TensorRT engine file\n",
    "        backend: Backend to use (\"tensorrt\" or \"onnxruntime\")\n",
    "        fp16_mode: Whether to convert to fp16\n",
    "        repository_base: Base path for model repository\n",
    "        \n",
    "    Returns:\n",
    "        Path to created model repository\n",
    "    \"\"\"\n",
    "    print(\"Creating triton model repository...\")\n",
    "    \n",
    "    # Validate backend argument\n",
    "    valid_backends = [\"tensorrt\", \"onnxruntime\"]\n",
    "    if backend not in valid_backends:\n",
    "        raise ValueError(f\"Backend must be one of {valid_backends}, got: {backend}\")\n",
    "\n",
    "    # Check that the model is in the provided repository\n",
    "    if Path(source_model_path).exists():\n",
    "        print(f\"Model source available: {source_model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No valid model files found (ONNX or TensorRT) at {source_model_path}\")\n",
    "    \n",
    "    # Create repository directory structure\n",
    "    model_file = \"model.onnx\"\n",
    "    repo_path = f\"{repository_base}/{backend}/triton_model_repository\"\n",
    "    model_path = f\"{repo_path}/{model_name}\"\n",
    "    version_path = f\"{model_path}/1\"\n",
    "    \n",
    "    print(f\"Creating repository structure: {repository_base}\")\n",
    "    Path(version_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy model file to repository\n",
    "    dest_path = f\"{version_path}/{model_file}\"\n",
    "    shutil.copy(source_model_path, dest_path)\n",
    "    \n",
    "    # Verify copy\n",
    "    source_size = Path(source_model_path).stat().st_size\n",
    "    dest_size = Path(dest_path).stat().st_size\n",
    "    if source_size != dest_size:\n",
    "        raise RuntimeError(f\"Model file copy failed: {source_size} != {dest_size} bytes\")\n",
    "    print(f\"Model copied: {source_size / 1024 / 1024:.1f}MB\")\n",
    "    \n",
    "    # TODO: Complete the Triton configuration file\n",
    "    # HINT: Find info at https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html.\n",
    "    # At minimum, the configuration should contain these fields: name, platform, max_batch_size, version_policy, input, output, and instance_group. \n",
    "    # Consider advanced settings like dynamic_batching and model_warmup too.\n",
    "    # For dynamic_bathching, the `preferred_batch_size` array should reflect your expected traffic patterns and correctly take advantage of the GPU hardware (remember: power of two). \n",
    "    # For model warmup, this can help prevens cold start latency during the first inference requests. Including multiple batch sizes ensures optimal \n",
    "    # performance across different traffic patterns in a multi-tenant environment.\n",
    "    config = f''''''  # Add your code here\n",
    "\n",
    "    # TensorRT optimization (only for TensorRT backend)\n",
    "    if backend==\"tensorrt\":\n",
    "        optimization_config = f'''\n",
    "optimization {{\n",
    "  execution_accelerators {{\n",
    "    gpu_execution_accelerator [\n",
    "      {{\n",
    "        name: \"tensorrt\"\n",
    "        parameters {{\n",
    "          key: \"precision_mode\"\n",
    "          value: \"{'FP16' if fp16_mode else 'FP32'}\"\n",
    "        }}\n",
    "        parameters {{\n",
    "          key: \"max_workspace_size_bytes\"\n",
    "          value: \"100000000\"\n",
    "        }}\n",
    "      }}\n",
    "    ]\n",
    "  }}\n",
    "}}\n",
    "'''\n",
    "        config+=optimization_config\n",
    "    \n",
    "    # Save configuration file\n",
    "    config_path = f\"{model_path}/config.pbtxt\"\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(config)\n",
    "    print(f\"Configuration created at {config_path}\")\n",
    "    \n",
    "    # Display final configuration\n",
    "    print(f\"\\nMODEL REPOSITORY SUMMARY:\")\n",
    "    print(f\"   Repository: {repository_base}\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Platform: {platform}\")\n",
    "    print(f\"   Backend: ONNX Runtime with TensorRT acceleration\")\n",
    "    print(f\"   Model file: {model_file} ({source_size / 1024 / 1024:.1f}MB)\")\n",
    "    print(f\"   Config: config.pbtxt ({Path(config_path).stat().st_size} bytes)\")\n",
    "    \n",
    "    return repo_path\n",
    "\n",
    "\n",
    "# Define Triton model repository\n",
    "print(f\"\\nCreating repository for Triton...\")\n",
    "repo_path = create_triton_model_repository(\n",
    "    model_name=model_name,\n",
    "    source_model_path=onnx_model_path,\n",
    "    backend=backend\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Deploy the Triton server\n",
    "\n",
    "Deploying the Triton server involves configuring and running an NVIDIA Triton Inference Server container using Docker. The server can handle different types of machine learning models, providing both HTTP and gRPC endpoints for inference requests. \n",
    "\n",
    "We provide a starter code in Python here, to avoid you spending too much time setting up with Docker. Instead, your focus will be on how to configure the Triton server.\n",
    "\n",
    "IMPORTANT: Pay attention to memory limits, GPU access, and port mappings. The configuration should balance resource allocation with the multi-tenant memory budget requirements.\n",
    "\n",
    "_Note that you could also perform these actions using CLI._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def deploy_triton_server(\n",
    "    repository_path: str,\n",
    "    max_wait_time: int = 120\n",
    ") -> Optional[object]:\n",
    "    \"\"\"\n",
    "    Deploy NVIDIA Triton Inference Server with production-ready configuration.\n",
    "    \n",
    "    Args:\n",
    "        repository_path: Path to Triton model repository\n",
    "        max_wait_time: Maximum wait time for server startup\n",
    "        \n",
    "    Returns:\n",
    "        container: Docker container object or None if deployment fails\n",
    "        ports: Dict of port type to value for http_port, grcp_port, and metrics_port\n",
    "    \"\"\"\n",
    "    print(\"Deploying Triton Inference Server...\")\n",
    "    \n",
    "    try:\n",
    "        # Define the docker client\n",
    "        client = docker.from_env()\n",
    "    \n",
    "        # Pre-requisite: Stop any existing Triton containers\n",
    "        existing_containers = client.containers.list(all=True)\n",
    "        for container in existing_containers:\n",
    "            if any('tritonserver' in tag.lower() for tag in container.image.tags):\n",
    "                try:\n",
    "                    print(f\"   Stopping existing container: {container.name}\")\n",
    "                    container.stop()\n",
    "                    container.remove()\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # TODO: Add missing entries in the configuration below for the container deployment\n",
    "        # Note that, only for the first run, the function could take up to 5 minutes to download the image\n",
    "        # HINT: `container_config` defines the configuration for the Docker image of Triton Inference Server. You can find information\n",
    "        # on available options at https://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.ContainerCollection.run.  \n",
    "        # IMPORTANT: \n",
    "        # 1. Choose the right Triton Inference image for the environment by looking at the support matrix [here](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-25-06.html#rel-25-06). You can find out your CUDA version by running `nvidia-smi` from command line.\n",
    "        # 2. At minimum, you should populate image, command, ports, volumes, runtime, detach: True\n",
    "        # 3. Consider a good `mem_limit` for the current T4 machine you are on\n",
    "        ports = {\"http_port\": 8000, \"grpc_port\": 8001, \"metrics_port\": 8002}\n",
    "        container_config = {}  # Add your code here\n",
    "\n",
    "        # Start the container with configuration\n",
    "        print(f\"Starting container...\")\n",
    "        container = client.containers.run(**container_config)\n",
    "        print(f\"    Container started: {container.short_id}\")\n",
    "        \n",
    "        # Wait for the server to be ready by checking the health endpoint\n",
    "        # Note that if you didn't set `strict-readiness=true`, you'd also need to wait for the `/models` endpoint to become complete\n",
    "        print(f\"Waiting for server to be ready...\")\n",
    "        for i in range(max_wait_time):\n",
    "            try:\n",
    "                health_endpoint = f\"http://localhost:{ports[\"http_port\"]}/v2/health/ready\"\n",
    "                response = requests.get(health_endpoint, timeout=2)\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"   Server is ready!\")\n",
    "                    return container, ports\n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                print(f\"      Still waiting... ({i+1}s)\")\n",
    "        \n",
    "        # If we get here, server didn't start properly\n",
    "        print(f\"   ERROR: Server failed to start within {max_wait_time} seconds\")\n",
    "        print(f\"   Check container logs for issues:\")\n",
    "        print(f\"      docker logs {container.logs().decode('utf-8')}\")\n",
    "        \n",
    "        # Clean up failed attempt\n",
    "        try:\n",
    "            container.stop(timeout=5)\n",
    "            container.remove()\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        return None\n",
    "            \n",
    "    except docker.errors.DockerException as e:\n",
    "        print(f\"ERROR: Docker operation failed\")\n",
    "        print(f\"DIAGNOSIS: {str(e)}\")\n",
    "        print(f\"SOLUTION: \")\n",
    "        print(f\"   1. Ensure Docker is running\")\n",
    "        print(f\"   2. Check if you have GPU runtime installed\")\n",
    "        print(f\"   3. Try CLI deployment instead\")\n",
    "        return None\n",
    "\n",
    "# Deploy Triton server\n",
    "# TODO: Define the maximum wait time for the server to become ready\n",
    "# Hint: Duration is dependant on the defined kick-off configuration processes - note that if the container errors, you will have to wait this time before the function exits\n",
    "# So try to select a good wait time which is not too aggressive nor too leniant (somewhere in between 1-5 minutes)\n",
    "max_wait_time =  # Int - Add your value here\n",
    "container, ports = deploy_triton_server(\n",
    "    repository_path=repo_path,\n",
    "    max_wait_time=150 \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Create the Triton client\n",
    "\n",
    "The `TritonClient` can be used to handle inference and gather metrics for the created server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonClient:\n",
    "    \"\"\"Client for communicating with NVIDIA Triton Inference Server.\"\"\"\n",
    "    \n",
    "    def __init__(self, server_url: str = \"localhost:8000\", model_name: str = \"udacimed_pneumonia_production\"):\n",
    "        self.server_url = server_url\n",
    "        self.model_name = model_name\n",
    "        self.base_url = f\"http://{server_url}/v2\"\n",
    "        self.model_endpoint = f\"{self.base_url}/models/{self.model_name}\"\n",
    "        \n",
    "        # Test connection to Triton server through model endpoint\n",
    "        try:\n",
    "            response = requests.get(f\"{self.model_endpoint}/ready\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Connected to Triton server. Model endpoint ready at {self.model_endpoint}\")\n",
    "            else:\n",
    "                print(f\"WARNING: Triton server responded, but the model endpoint at {self.model_endpoint} is not ready: {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ERROR: Cannot connect to the model endpoint of Triton server\")\n",
    "            print(f\"DIAGNOSIS: {str(e)}\")\n",
    "            print(f\"SOLUTION: \")\n",
    "            print(f\"   1. Ensure Triton server is running\")\n",
    "            print(f\"   2. Check if you have set the right port for the `server_url`\")\n",
    "            print(f\"   3. Verify model is loaded, or needs explicit loading: {self.model_name}\")\n",
    "    \n",
    "    def infer(self, input_data: np.ndarray, fp16_mode: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform inference via Triton server.\n",
    "        \n",
    "        Args:\n",
    "            input_data: Input tensor as numpy array [batch_size, 3, 64, 64]\n",
    "            \n",
    "        Returns:\n",
    "            Model predictions as numpy array [batch_size, 2]\n",
    "        \"\"\"\n",
    "        # TODO: Prepare inference request\n",
    "        # HINT: Triton expects JSON format that follows the KServe community standard inference protocol\n",
    "        # See: https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/protocol/README.html\n",
    "        request_data = {}  # Add your code here\n",
    "        \n",
    "        # Send inference request using POST to the inference endpoint\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.model_endpoint}/infer\",\n",
    "                json=request_data,\n",
    "                headers={\"Content-Type\": \"application/json\"}\n",
    "            )\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                raise RuntimeError(f\"Inference failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "            # Extract results from response\n",
    "            result = response.json()\n",
    "            output_data = np.array(result[\"outputs\"][0][\"data\"])\n",
    "            output_shape = result[\"outputs\"][0][\"shape\"]\n",
    "            \n",
    "            return output_data.reshape(output_shape)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"ERROR: Inference request failed\")\n",
    "            print(f\"DIAGNOSIS: {str(e)}\")\n",
    "            print(f\"SOLUTION: \")\n",
    "            print(f\"   1. Check if Triton server is running\")\n",
    "            print(f\"   2. Verify model is loaded and ready\")\n",
    "            print(f\"   3. Check input data format and dimensions\")\n",
    "            raise\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get Triton server performance metrics, automatically served by Triton's metrics endpoint.\n",
    "        The endpoint provides Prometheus metrics indicating GPU and request statistics.  \n",
    "\n",
    "        This function returns araw dictionary of metrics with value, extracted from the Prometheus format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            metrics_endpoint = f\"http://localhost:{ports[\"metrics_port\"]}/metrics\"\n",
    "            response = requests.get(metrics_endpoint)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the raw Prometheus text output\n",
    "                raw_metrics = {}\n",
    "                \n",
    "                for family in text_string_to_metric_families(response.text):\n",
    "                    for sample in family.samples:\n",
    "                        metric_name = sample.name\n",
    "                        raw_metrics[metric_name] = sample.value\n",
    "\n",
    "                if not raw_metrics:\n",
    "                    print(\"WARNING: No metrics available from Triton server\")\n",
    "                    return {}\n",
    "\n",
    "                return raw_metrics\n",
    "            else:\n",
    "                return {}\n",
    "        except:\n",
    "            print(\"WARNING: Metrics endpoint not available! Check the `get_metrics()` method from TritonClient\")\n",
    "            return {}\n",
    "\n",
    "# Create Triton client\n",
    "# Uses the same model name as defined in the repository creation\n",
    "triton_client = TritonClient(model_name=model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Benchmark model performance on all metrics\n",
    "\n",
    "Now it's time to perform benchmarking of the complete optimization and deployment pipeline.\n",
    "\n",
    "While PyTorch local metrics used so far measure pure model inference (GPU compute only), Triton Server metrics include network overhead, queueing, batching, and model execution. As such, Triton metrics are more representative of real-world deployment performance and are preferrable for benchmarking as they provide:\n",
    "- **Accuracy**: Server-side measurements are more precise\n",
    "- **Standardization**: Industry-standard Prometheus format\n",
    "- **Production monitoring**: Same metrics used for alerting and scaling\n",
    "- **Comprehensive data**: Includes queue time, compute time, batch efficiency\n",
    "\n",
    "TODO: Read about all [available metrics from the Triton Server Inference metrics endpoint](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/metrics.html) before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics from Triton Server metrics\n",
    "def calculate_benchmark_metrics(initial_metrics: Dict, final_metrics: Dict, batch_size: int = 1) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate performance metrics per request from before/after Triton metrics snapshots, i.e., for the current inference run.\n",
    "    This is necessary because Triton metrics are cumulative across all inference runs.\n",
    "    \n",
    "    Args:\n",
    "        initial_metrics: Raw metrics before benchmark\n",
    "        final_metrics: Raw metrics after benchmark\n",
    "        \n",
    "    Returns:\n",
    "        Calculated performance metrics for the benchmark period only\n",
    "    \"\"\"\n",
    "    # Calculate deltas between snapshots\n",
    "    # final_value - initial_value gives metrics for benchmark period only\n",
    "    # This is because metrics are cumulative from the server across all runs, so you need to extract deltas from before and after benchmark\n",
    "    delta_requests = final_metrics.get('nv_inference_request_success_total', 0) - initial_metrics.get('nv_inference_request_success_total', 0)\n",
    "    delta_request_time_us = final_metrics.get('nv_inference_request_duration_us_total', 0) - initial_metrics.get('nv_inference_request_duration_us_total', 0)\n",
    "    delta_queue_time_us = final_metrics.get('nv_inference_queue_duration_us_total', 0) - initial_metrics.get('nv_inference_queue_duration_us_total', 0)\n",
    "    delta_compute_input_us = final_metrics.get('nv_inference_compute_input_duration_us_total', 0) - initial_metrics.get('nv_inference_compute_input_duration_us_total', 0)\n",
    "    delta_compute_infer_us = final_metrics.get('nv_inference_compute_infer_duration_us_total', 0) - initial_metrics.get('nv_inference_compute_infer_duration_us_total', 0)\n",
    "    delta_compute_output_us = final_metrics.get('nv_inference_compute_output_duration_us_total', 0) - initial_metrics.get('nv_inference_compute_output_duration_us_total', 0)\n",
    "    \n",
    "    if delta_requests == 0:\n",
    "        print(\"ERROR: No new requests detected between snapshots\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculate per-request averages from deltas\n",
    "    # delta_total_time / delta_requests = average per request for benchmark period (+ transform from us to ms!)\n",
    "    avg_request_time_ms = (delta_request_time_us / delta_requests) / 1000\n",
    "    avg_queue_time_ms = (delta_queue_time_us / delta_requests) / 1000\n",
    "    avg_compute_input_ms = (delta_compute_input_us / delta_requests) / 1000\n",
    "    avg_compute_infer_ms = (delta_compute_infer_us / delta_requests) / 1000\n",
    "    avg_compute_output_ms = (delta_compute_output_us / delta_requests) / 1000\n",
    "    avg_compute_total_ms = avg_compute_input_ms + avg_compute_infer_ms + avg_compute_output_ms\n",
    "    \n",
    "    # TODO: Calculate throughput from average latency\n",
    "    throughput_requests_per_sec =  # Add your code here\n",
    "    \n",
    "    # TODO: Get current memory usage\n",
    "    # HINT: Not delta - this is current state\n",
    "    gpu_memory_mb =  # Add your code here\n",
    "    \n",
    "    metrics = {\n",
    "        # Request duration metrics (end-to-end latency including network overhead)\n",
    "        'request_latency_ms': avg_request_time_ms,\n",
    "        \n",
    "        # Queue duration (time spent waiting for batching)\n",
    "        'request_queue_time_ms': avg_queue_time_ms,\n",
    "        \n",
    "        # Pure compute time breakdown (closest to PyTorch local measurements)\n",
    "        'request_compute_input_ms': avg_compute_input_ms,\n",
    "        'request_compute_infer_ms': avg_compute_infer_ms,\n",
    "        'request_compute_output_ms': avg_compute_output_ms,\n",
    "        'request_compute_total_ms': avg_compute_total_ms,\n",
    "        \n",
    "        # GPU memory usage (current deployment memory footprint)\n",
    "        'memory_used_mb': gpu_memory_mb,\n",
    "        \n",
    "        # Throughput and request stats\n",
    "        'throughput_requests_per_sec': throughput_requests_per_sec,\n",
    "        'total_successful_requests': delta_requests\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def benchmark_triton_performance(triton_client: TritonClient, \n",
    "                                test_data: torch.Tensor,\n",
    "                                num_single_requests: int = 50,\n",
    "                                num_batch_requests: int = 25,\n",
    "                                batch_size: int = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmarking function with separate single-sample and batch performance analysis.\n",
    "    \n",
    "    Args:\n",
    "        triton_client: Triton inference client\n",
    "        test_data: Test input tensors for benchmarking\n",
    "        num_single_requests: Number of single-sample requests to send\n",
    "        num_batch_requests: Number of batch requests to send\n",
    "        batch_size: Batch size for batch requests (defaults to test_data batch size)\n",
    "        \n",
    "    Returns:\n",
    "        Performance metrics with separate single-sample and batch analysis\n",
    "    \"\"\"\n",
    "    print(f\"Benchmarking Triton deployment performance with single/batch analysis...\")\n",
    "    \n",
    "    # Pre-requisite: Prepare test data for inference\n",
    "\n",
    "    # TODO: Set up data on the right device and with the right numpy inference type\n",
    "    # HINT: The single_sample can just be one item from test_batch\n",
    "    test_batch =  # Add your code here\n",
    "    single_sample = test_batch[:1]\n",
    "    \n",
    "    # Configure batch size\n",
    "    if batch_size is None:\n",
    "        batch_size = test_batch.shape[0]\n",
    "        batch_data = test_batch\n",
    "    else:\n",
    "        # Create batch with specified size (repeat if necessary)\n",
    "        if batch_size <= test_batch.shape[0]:\n",
    "            batch_data = test_batch[:batch_size]\n",
    "        else:\n",
    "            # Repeat samples to reach desired batch size\n",
    "            repeats = (batch_size + test_batch.shape[0] - 1) // test_batch.shape[0]\n",
    "            repeated_data = np.tile(test_batch, (repeats, 1, 1, 1))\n",
    "            batch_data = repeated_data[:batch_size]\n",
    "    \n",
    "    print(f\"   Single-sample requests: {num_single_requests}\")\n",
    "    print(f\"   Batch requests: {num_batch_requests} (batch_size={batch_size})\")\n",
    "    \n",
    "    # Phase 1 - Initial metrics snapshot\n",
    "    print(\"   Taking initial metrics snapshot...\")\n",
    "    initial_metrics = triton_client.get_metrics()\n",
    "    \n",
    "    if not initial_metrics:\n",
    "        return {'benchmark_success': False, 'error': 'Could not collect initial metrics'}\n",
    "    \n",
    "    initial_count = initial_metrics.get('nv_inference_request_success_total', 0)\n",
    "    print(f\"   Initial request count: {initial_count}\")\n",
    "    \n",
    "    try:\n",
    "        # Phase 2 - Single-sample inference\n",
    "        print(\"  Phase 1: Single-sample inference...\")\n",
    "        for i in range(num_single_requests):\n",
    "            # TODO: Run inference on a single sample\n",
    "            # HINT: You can use the triton_client object on single_sample\n",
    "            _ = # Add your code here\n",
    "            if i % 20 == 19:\n",
    "                print(f\"      Progress: {i+1}/{num_single_requests}\")\n",
    "        time.sleep(2)  # Wait for metrics to update\n",
    "\n",
    "        print(\"   Taking post-single-sample metrics snapshot...\")\n",
    "        post_single_metrics = triton_client.get_metrics()\n",
    "        \n",
    "        if not post_single_metrics:\n",
    "            return {'benchmark_success': False, 'error': 'Could not collect post-single metrics'}\n",
    "        \n",
    "        post_single_count = post_single_metrics.get('nv_inference_request_success_total', 0)\n",
    "        single_requests_processed = post_single_count - initial_count\n",
    "        print(f\"   Single-sample requests processed: {single_requests_processed}\")\n",
    "        \n",
    "        # Phase 3 - Batch inference\n",
    "        print(f\" Phase 2: Batch inference with batch size {batch_size}...\")\n",
    "        for i in range(num_batch_requests):\n",
    "            # TODO: Run inference on a batch sample\n",
    "            # HINT: You can use the triton_client object on batch_data\n",
    "            _ = # Add your code here\n",
    "            if i % 10 == 9:\n",
    "                print(f\"      Progress: {i+1}/{num_batch_requests}\")\n",
    "        \n",
    "        time.sleep(2)  # Wait for metrics to update\n",
    "        \n",
    "        print(\"   Taking final metrics snapshot...\")\n",
    "        final_metrics = triton_client.get_metrics()\n",
    "        \n",
    "        if not final_metrics:\n",
    "            return {'benchmark_success': False, 'error': 'Could not collect final metrics'}\n",
    "        \n",
    "        final_count = final_metrics.get('nv_inference_request_success_total', 0)\n",
    "        batch_requests_processed = final_count - post_single_count\n",
    "        total_requests_processed = final_count - initial_count\n",
    "        \n",
    "        print(f\"   Batch requests processed: {batch_requests_processed}\")\n",
    "        print(f\"   Total requests processed: {total_requests_processed}\")\n",
    "        \n",
    "        if single_requests_processed == 0 and batch_requests_processed == 0:\n",
    "            print(\"WARNING: No requests detected in either phase\")\n",
    "            return {'benchmark_success': False, 'error': 'No metric updates detected'}\n",
    "        \n",
    "        # Calculate performance metrics for both phases\n",
    "        single_sample_metrics = {}\n",
    "        batch_metrics = {}\n",
    "        \n",
    "        # Calculate single-sample metrics\n",
    "        if single_requests_processed > 0:\n",
    "            print(\"   Calculating single-sample performance...\")\n",
    "            single_sample_metrics = calculate_benchmark_metrics(initial_metrics, post_single_metrics)\n",
    "            if single_sample_metrics:\n",
    "                single_sample_metrics['phase'] = 'single_sample'\n",
    "                single_sample_metrics['samples_per_request'] = 1\n",
    "                \n",
    "        # Calculate batch metrics  \n",
    "        if batch_requests_processed > 0:\n",
    "            print(\"   Calculating batch performance...\")\n",
    "            batch_metrics = calculate_benchmark_metrics(post_single_metrics, final_metrics)\n",
    "            if batch_metrics:\n",
    "                batch_metrics['phase'] = 'batch'\n",
    "                batch_metrics['samples_per_request'] = batch_size\n",
    "                if batch_metrics.get('request_latency_ms', 0) > 0:\n",
    "                    batch_metrics['request_compute_total_ms'] = batch_metrics['request_compute_total_ms'] / batch_size\n",
    "                    batch_metrics['per_sample_latency_ms'] = batch_metrics['request_latency_ms'] / batch_size\n",
    "                    batch_metrics['per_sample_throughput'] = batch_metrics['throughput_requests_per_sec'] * batch_size\n",
    "        \n",
    "        # Calculate efficiency comparison\n",
    "        efficiency_analysis = {}\n",
    "        if single_sample_metrics and batch_metrics:\n",
    "            single_latency = single_sample_metrics.get('request_latency_ms', 0)\n",
    "            batch_per_sample_latency = batch_metrics.get('per_sample_latency_ms', 0)\n",
    "            \n",
    "            if single_latency > 0 and batch_per_sample_latency > 0:\n",
    "                efficiency_analysis = {\n",
    "                    'batch_efficiency_ratio': single_latency / batch_per_sample_latency,\n",
    "                    'single_sample_latency_ms': single_latency,\n",
    "                    'batch_per_sample_latency_ms': batch_per_sample_latency,\n",
    "                    'batch_size': batch_size,\n",
    "                    'latency_improvement_percent': ((single_latency - batch_per_sample_latency) / single_latency) * 100,\n",
    "                    'throughput_improvement_ratio': batch_metrics.get('per_sample_throughput', 0) / single_sample_metrics.get('throughput_samples_per_sec', 1)\n",
    "                }\n",
    "        \n",
    "        print(f\"Benchmark completed successfully!\")\n",
    "        return {\n",
    "            'benchmark_success': True,\n",
    "            'single_sample_metrics': single_sample_metrics,\n",
    "            'batch_metrics': batch_metrics,\n",
    "            'efficiency_analysis': efficiency_analysis,\n",
    "            'summary': {\n",
    "                'single_requests_processed': single_requests_processed,\n",
    "                'batch_requests_processed': batch_requests_processed,\n",
    "                'total_requests_processed': total_requests_processed,\n",
    "                'configured_batch_size': batch_size\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Benchmark failed - {str(e)}\")\n",
    "        return {'benchmark_success': False, 'error': str(e)}\n",
    "\n",
    "# Run the benchmark\n",
    "# TODO: Choose the batch size\n",
    "# HINT: You may want to start with the optimal batch_size from the batch analysis in notebook 2, and then experiment with other values too!\n",
    "batch_size = # Int - Add your value here\n",
    "benchmark_results = benchmark_triton_performance(\n",
    "    triton_client=triton_client,\n",
    "    test_data=sample_images,\n",
    "    num_single_requests=30,       # Focused single-sample testing\n",
    "    num_batch_requests=20,        # Focused batch testing\n",
    "    batch_size=batch_size         # Parameterized batch size\n",
    ")\n",
    "pprint(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Performance on latency and throughput is worse than expected?** Make sure to handle cold start in the `config.pbtxt` for the chosen batch_size! Or, run this cell multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Have you noticed something about GPU memory?**\n",
    "> \n",
    "> Your optimized model should be <100MB, but Triton shows a few hundred megabytes for model size (once benchmarking is completed). What do you think is causing this huge difference? Is this ALL model memory, or something else? \n",
    "> \n",
    "> HINT: Triton metrics endpoint reports allocated GPU memory for the complete Triton process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use NVIDIA Triton Model Analyzer to check the memory footprint by model\n",
    "# Follow the instructions below to install and run model analysis, and manually extract the model memory from generate reports\n",
    "benchmark_results['model_memory_mb'] = # Float - Add your value here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**---NVIDIA Triton Model Analyzer---**\n",
    "\n",
    "1. Open a local terminal window.\n",
    "\n",
    "2. Run the model analyzer using the recommended approach from https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_analyzer/docs/install.html:\n",
    "\n",
    "```\n",
    " # Get the absolute path first\n",
    " MODEL_REPO_PATH=$(realpath $(pwd)/../deployment/onnxruntime/triton_model_repository)\n",
    " RESULTS_PATH=$(realpath $(pwd)/../deployment/onnxruntime/triton_model_analyzer)\n",
    "\n",
    " docker run --rm -it \\\n",
    "     --net=host \\\n",
    "     --gpus all \\\n",
    "     -e MODEL_REPO_PATH=\"${MODEL_REPO_PATH}\" \\\n",
    "     -e RESULTS_PATH=\"${RESULTS_PATH}\" \\\n",
    "     -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "     -v ${MODEL_REPO_PATH}:${MODEL_REPO_PATH}:ro \\\n",
    "     -v ${RESULTS_PATH}:${RESULTS_PATH} \\\n",
    "     nvcr.io/nvidia/tritonserver:{TODO: Add your chosen triton version}-sdk\n",
    "```\n",
    "\n",
    "<br>Note that we are using the same Triton Server image, but with the `-sdk` to ensure model-analyzer is available.\n",
    "\n",
    "3. Run the model analyzer with relevant flags:\n",
    "\n",
    "```\n",
    " pip install --upgrade requests==2.31.0  # Solve the DockerException for http+docker scheme\n",
    "\n",
    " model-analyzer profile \\\n",
    "     --model-repository ${MODEL_REPO_PATH} \\\n",
    "     --profile-models udacimed_pneumonia_production \\\n",
    "     --triton-launch-mode=docker \\\n",
    "     --triton-http-endpoint localhost:9000 \\\n",
    "     --triton-grpc-endpoint localhost:9001 \\\n",
    "     --triton-metrics-url http://localhost:9002/metrics \\\n",
    "     --gpus 0 \\\n",
    "     --output-model-repository-path ${RESULTS_PATH}/output \\\n",
    "     --export-path=${RESULTS_PATH} \\\n",
    "     --override-output-model-repository \\\n",
    "     --run-config-search-disable \\\n",
    "     --concurrency 1 \\\n",
    "     --batch-sizes 1,32\n",
    "```\n",
    "\n",
    "<br>Note that:\n",
    "\n",
    "- Different ports are needed than those from the running container in this notebook (to keep the latter running).\n",
    "- You should create a `config.yaml` instead for reproducibility in production.\n",
    "<br>\n",
    "\n",
    "4. Go to `$RESULTS_PATH/results`, and define model memory usage by subtracting **GPU Memory Usage (MB)** in `metrics-server-only.csv` from **GPU Memory Usage (MB)** in `metrics-model-gpu.csv`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **When to use Metrics Endpoint vs Model Analyzer with Triton?**\n",
    "> \n",
    "> Model Analyzer is used for production testing. It Identifies bottlenecks under real-world conditions - concurrent clients, varying loads, resource contention. \n",
    "> Metrics Endpoints is used for live monitoring of operational metrics, for production alerting and scaling decisions. Perfect for validating our optimization targets.\n",
    "> \n",
    "> For a real-world deployment, we'd use Model Analyzer for thorough production testing, plus dedicated dev/QA testing to validate performance (and targets being met!) under UdaciMed's multi-tenant concurrent client scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate clinical performance\n",
    "def validate_clinical_performance_simple(triton_client: TritonClient,\n",
    "                                        test_loader,\n",
    "                                        threshold: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Clinical validation using test data.\n",
    "    \n",
    "    Args:\n",
    "        triton_client: Triton inference client\n",
    "        test_loader: Test dataset loader\n",
    "        threshold: Decision threshold for classification\n",
    "        \n",
    "    Returns:\n",
    "        Clinical performance metrics from Triton deployment\n",
    "    \"\"\" \n",
    "    print(\"Validating clinical performance on test data...\")\n",
    "    \n",
    "    triton_predictions = []\n",
    "    true_labels = []\n",
    "    samples_processed = 0\n",
    "    \n",
    "    # TODO: Collect predictions from Triton inference into the variables initialized above\n",
    "    # HINT: Process batches from test_loader converted to numpy with the right input type, and collect predictions outputs\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    if len(triton_predictions) == 0:\n",
    "        print(\"ERROR: No successful predictions collected\")\n",
    "        return {'clinical_validation_success': False}\n",
    "    \n",
    "    # Convert to numpy arrays for metric calculation\n",
    "    predictions = np.array(triton_predictions)\n",
    "    labels = np.array(true_labels).flatten()\n",
    "    \n",
    "    # Calculate clinical metrics\n",
    "    pred_classes = (predictions > threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix components\n",
    "    tp = np.sum((pred_classes == 1) & (labels == 1))\n",
    "    fn = np.sum((pred_classes == 0) & (labels == 1))\n",
    "    tn = np.sum((pred_classes == 0) & (labels == 0))\n",
    "    fp = np.sum((pred_classes == 1) & (labels == 0))\n",
    "    \n",
    "    # Calculate clinical metrics\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"Clinical validation completed on {len(labels)} samples\")\n",
    "    return {\n",
    "        'clinical_validation_success': True,\n",
    "        'samples_validated': len(labels),\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'accuracy': accuracy,\n",
    "        'true_positives': tp,\n",
    "        'false_negatives': fn,\n",
    "        'confusion_matrix': {'tp': tp, 'fn': fn, 'tn': tn, 'fp': fp}\n",
    "    }\n",
    "\n",
    "# Validate clinical performance\n",
    "# TODO: Define threshold for binary classification\n",
    "clinical_threshold =  # Float - Add your value here\n",
    "clinical_results = validate_clinical_performance_simple(\n",
    "    triton_client=triton_client,\n",
    "    test_loader=test_loader,\n",
    "    threshold=clinical_threshold\n",
    ")\n",
    "pprint(clinical_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assess if production targets are met\n",
    "\n",
    "Final evaluation against all production deployment requirements. Meeting all targets demonstrates successful optimization for UdaciMed's deployment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define production targets\n",
    "# Feel free to skip FLOP reduction analysis: if TensorRT is enabled, simply assume 2-10% relative improvement\n",
    "PRODUCTION_TARGETS = {\n",
    "    'memory': 100,\n",
    "    'throughput': 2000, \n",
    "    'latency': 3,    \n",
    "    'sensitivity': 98, \n",
    "}\n",
    "\n",
    "DEPLOYMENT_VALUES = {\n",
    "    'memory': benchmark_results['model_memory_mb'] or benchmark_results['batch_metrics'].get('memory_used_mb'),\n",
    "    'throughput': {\n",
    "        benchmark_results['batch_metrics'].get('phase', 'batch'): benchmark_results['batch_metrics'].get('per_sample_throughput'),\n",
    "        benchmark_results['single_sample_metrics'].get('phase', 'single_sample'): benchmark_results['single_sample_metrics'].get('throughput_requests_per_sec'),\n",
    "    },\n",
    "    'latency': {\n",
    "        benchmark_results['batch_metrics'].get('phase', 'batch'): benchmark_results['batch_metrics'].get('request_latency_ms'),\n",
    "        benchmark_results['single_sample_metrics'].get('phase', 'single_sample'): benchmark_results['single_sample_metrics'].get('request_latency_ms'),\n",
    "    },\n",
    "    'sensitivity': clinical_results.get('sensitivity', 0)*100\n",
    "}\n",
    "\n",
    "print(\"Production deployment values vs targets:\")\n",
    "for metric, target in PRODUCTION_TARGETS.items():\n",
    "    print(f\"   {metric.replace('_', ' ').title()}: Target={target} --> @Deployment={DEPLOYMENT_VALUES[metric]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: TODO: Cross-platform deployment analysis\n",
    "\n",
    "Now that You have mastered GPU deployment with Triton, it's time to analyze how UdaciMed's pneumonia detection model would perform across different deployment environments. Healthcare systems have diverse infrastructure needs - from hospital workstations (CPU-only) to portable clinic devices and mobile health applications.\n",
    "\n",
    "> **Use case context**: UdaciMed serves hospitals with varying IT infrastructure. Some have modern GPU workstations, others rely on CPU-only systems, and many need portable solutions for rural clinics or emergency response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.1: Optimization strategy for CPU deployment\n",
    "\n",
    "Hospital workstations often lack dedicated GPUs but need to maintain clinical performance and multi-tenant efficiency. Let's analyze CPU deployment options for UdaciMed's hospital deployment!\n",
    "\n",
    "> **Numerical precision opportunities with GPU and CPU**: CPUs don't benefit from FP16 (most CPUs only emulate FP16). But CPUs supports another type of numerical optimization, remember?\n",
    "\n",
    "#### Analyze CPU deployment options\n",
    "\n",
    "Consider UdaciMed's requirements: <100MB memory budget, >98% sensitivity preservation, multi-tenant hospital deployment, and minimal clinical risk.\n",
    "\n",
    "_<<TODO: Complete the table below by filling in missing performance expectations, pros/cons for each approach>>_\n",
    "\n",
    "| Approach | Conversion Path | Memory Footprint | Performance | Clinical Risk | Multi-Tenant Support |\n",
    "|----------|----------------|------------------|-------------|---------------|---------------------|\n",
    "| **PyTorch on CPU** | Direct (no conversion) | High | Baseline | **Low** - same model | No batching |\n",
    "| **ONNX Runtime Default CPU** | | | | | |\n",
    "| **OpenVINO Runtime CPU** | | | | | |\n",
    "| **OpenVINO Backend for Triton** | | | | | |\n",
    "\n",
    "_<<TODO: Briefly answer the questions below based on UdaciMed's hospital deployment requirements>>_\n",
    "\n",
    "**1. Which approaches meet the memory budget requirement?**\n",
    "\n",
    "**2. Which approach poses the lowest clinical risk? Why?**\n",
    "\n",
    "**3. How important is dynamic batching for hospital workstations vs multi-tenant cloud deployment?**\n",
    "\n",
    "**4. What are the trade-offs between OpenVINO Runtime CPU and Triton+OpenVINO for UdaciMed?**\n",
    "\n",
    "#### Make your strategic choice\n",
    "\n",
    "Based on your analysis above, choose the best CPU deployment approach for UdaciMed:\n",
    "\n",
    "**My recommendation for UdaciMed's hospital CPU deployment:** \n",
    "\n",
    "_<<TODO: Choose one approach and justify your decision in 2-3 sentences>>_\n",
    "\n",
    "### Define an optimal CPU deployment configuration in OpenVINO\n",
    "\n",
    "Imagine you are testing out now CPU deployment with OpenVINO for UdaciMed, and set up the OpenVINO configuration to balance performance, memory, and clinical safety.\n",
    "\n",
    "_<<TODO: Complete the OpenVINO configuration below>>_\n",
    "\n",
    "```yaml\n",
    "# openvino_hospital_config.yaml\n",
    "# UdaciMed Hospital Workstation Deployment Configuration\n",
    "\n",
    "model_optimization:\n",
    "  input_model: \"udacimed_pneumonia_optimized.onnx\"\n",
    "  target_device: \"CPU\"\n",
    "  \n",
    "  # Choose precision strategy\n",
    "  precision: # TODO - Options: \"FP32\" (safe), \"FP16\", or \"INT8\" (faster, smaller, but clinical risk)\n",
    "  \n",
    "  # Set optimization priority  \n",
    "  optimization_level: # TODO - Options: \"ACCURACY\" (safe) or \"PERFORMANCE\" (faster)\n",
    "  \n",
    "  # Configure quantization (if using INT8)\n",
    "  quantization:\n",
    "    enabled:  # TODO: true/false\n",
    "    calibration_dataset_size:  # TODO - Number of samples for INT8 calibration (if enabled)\n",
    "\n",
    "deployment_config:\n",
    "  # Configure CPU utilization for hospital workstations\n",
    "  cpu_threads: # TODO - Options: 1, 2, 4, 8 (consider multi-tenancy impact)\n",
    "  \n",
    "  # Set memory allocation for multi-tenant deployment\n",
    "  memory_pool_mb: # TODO - Memory budget per model instance\n",
    "  \n",
    "  # Choose batching strategy\n",
    "  max_batch_size: # TODO - 1 (single patient) or higher (if implementing manual batching)\n",
    "  \n",
    "  # Configure for hospital network environment\n",
    "  inference_timeout_ms: # TODO: Maximum inference time before timeout\n",
    "\n",
    "clinical_validation:\n",
    "  # Define validation requirements after CPU deployment\n",
    "  sensitivity_threshold: # TODO: Minimum acceptable sensitivity (should be >98%)\n",
    "  validation_dataset_size: # TODO: Number of samples for clinical re-validation\n",
    "  comparison_baseline: \"GPU_Triton_deployment\"  # Compare against your GPU results\n",
    "```\n",
    "\n",
    "_<<TODO: Justify each configuration choice with one sentence>>_\n",
    "\n",
    "**Precision choice (FP32):**\n",
    "\n",
    "**Optimization level (ACCURACY):**\n",
    "\n",
    "**CPU threads (4):**\n",
    "\n",
    "**Memory allocation (80MB):**\n",
    "\n",
    "**Batch size (1):**\n",
    "\n",
    "**Clinical validation parameters (98.0% sensitivity, 1000 samples):**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2: Optimization strategy for mobile and edge deployment\n",
    "\n",
    "UdaciMed's vision extends beyond hospital workstations to portable devices and mobile health applications. This enables pneumonia detection in rural clinics, emergency response, and preventive screening programs where traditional infrastructure is limited.\n",
    "\n",
    "> **Mobile and edge requirementy**: These deployments require lightweight runtimes, offline capability, extended battery life, and often benefit from platform-specific optimizations. However, conversion complexity and clinical validation requirements vary significantly across approaches.\n",
    "\n",
    "#### Analyze mobile deployment options\n",
    "\n",
    " Consider UdaciMed's mobile/edge requirements: <50MB app size for developing markets, >98% sensitivity preservation, cross-platform reach for global health, offline capability for rural clinics, and minimal clinical validation burden.\n",
    "\n",
    "_<<TODO: Complete the table below by analyzing conversion complexity, clinical risk, and UdaciMed suitability for each approach>>_\n",
    "\n",
    "| Platform | Conversion Path | Model Size | Clinical Risk | Platform Coverage | Development Complexity | Edge Suitability |\n",
    "|----------|----------------|------------|---------------|-------------------|----------------------|------------------|\n",
    "| **ExecuTorch** | PyTorch→TorchScript→Mobile | ~25MB | **Low** - single conversion | Cross-platform | **Low** - familiar workflow | **High** - offline capable |\n",
    "| **LiteRT** | | | | | | |\n",
    "| **Core ML (iOS)** | | | | | | |\n",
    "| **ONNX Runtime Mobile** | | | | | | |\n",
    "\n",
    "_<<TODO: Answer the questions below based on UdaciMed's mobile and edge deployment strategy>>_\n",
    "\n",
    "**1. Which approaches meet the app size requirement (<50MB total) and are suitable for offline edge deployment?**\n",
    "\n",
    "**2. Which conversion path poses the lowest clinical risk for UdaciMed's >98% sensitivity requirement?**\n",
    "\n",
    "**3. Should UdaciMed prioritize cross-platform reach or platform-specific optimization for global health impact?**\n",
    "\n",
    "**4. How do development complexity and regulatory validation burden affect UdaciMed's resource allocation?**\n",
    "\n",
    "**5. Which approaches best support offline deployment in rural clinics and emergency response scenarios?**\n",
    "\n",
    "**6. What are the power consumption and battery life implications for portable clinic devices?**\n",
    "\n",
    "#### Make your mobile/edge strategy choice\n",
    "\n",
    "**My recommendation for UdaciMed's mobile and edge deployment strategy:**\n",
    "\n",
    "_<<TODO: Choose one approach and justify your decision in 2-3 sentences, considering clinical risk, development resources, and global health reach>>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!**\n",
    "\n",
    "You have successfully implemented a complete hardware-accelerated deployment pipeline! Let's recap the decisions you have made and results you have achieved while transforming an optimized model into a production-ready healthcare solution.\n",
    "\n",
    "### **Production deployment scorecard**\n",
    "\n",
    "**Final GPU deployment performance vs UdaciMed targets:**\n",
    "\n",
    "_<<TODO: Complete final scorecard based on your benchmarking results:>>_\n",
    "\n",
    "| Metric | Target | Achieved | Status |\n",
    "|--------|--------|----------|--------|\n",
    "| **Memory Usage** | <100MB | | |\n",
    "| **Throughput** | >2,000 samples/sec | | |\n",
    "| **Latency** | <3ms | | | |\n",
    "| **FLOP Reduction** | >80% | | | |\n",
    "| **Clinical Safety** | >98% sensitivity | | | |\n",
    "\n",
    "_<<TODO: Give yourself a final production score given the number of targets met>>_\n",
    "\n",
    "**Overall production score: X/5 targets met!**\n",
    "\n",
    "### **Strategic deployment insights**\n",
    "\n",
    "_<<TODO: Reflect on the key decisions you made, and why>>_\n",
    "\n",
    "#### Mixed Precision Strategy\n",
    "**Your FP16/FP32 choice:** # _(FP32, FP16)_\n",
    "\n",
    "**Why you made this decision:**\n",
    "\n",
    "#### Backend Selection\n",
    "**Your Triton backend choice:**  # _(ONNX Runtime, TensorRT, etc.)_\n",
    "\n",
    "**Why this backend aligned with UdaciMed's requirements:**\n",
    "\n",
    "#### Batching Configuration\n",
    "**Your dynamic batching setup:** # _(preferred batch sizes, queue delay, etc.)_\n",
    "\n",
    "**How this supports multi-tenant deployment:** \n",
    "\n",
    "### Optimization philosophy\n",
    "**Meeting targets vs maximizing metrics:**\n",
    "\n",
    "_<<TODO: What did you learn about when to stop optimizing and why?>>_\n",
    "\n",
    "---\n",
    "\n",
    "**You have completed the full journey from architectural optimization to production-ready deployment, demonstrating the technical skills and strategic thinking essential for deploying AI in healthcare. Your UdaciMed pneumonia detection system is now ready to serve hospitals worldwide while maintaining the clinical safety standards that save lives.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
